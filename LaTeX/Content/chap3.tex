In this section, we deal with the case of no detection ambiguity. Therefore, we add the following, more restrictive assumptions, to those presented in Assumption~\ref{ass:general_assumption}.

%%% IEEE %%%
\begin{assumption}\label{ass:basic_assumptions}
\leavevmode
\begin{enumerate}[(i)]
\item The sensor generates exactly one detection for each target
 at each scan i.e., no missed detections.
\item The sensor does not generate any additional detections
i.e., no false alarms.
\end{enumerate}
\end{assumption}

%%% Thesis %%%
%\begin{assumption}\label{ass:basic_assumptions}
%\begin{minipage}[t]{\columnwidth}
%\begin{enumerate}[(i)]
%\item The sensor generates exactly one detection for each target\\
% at each scan i.e., no missed detections.
%\item The sensor does not generate any additional detections\\
%i.e., no false alarms.
%\end{enumerate}
%\end{minipage}
%\end{assumption}

We begin constructing our MIO model by introducing decision variables that define data associations as well as estimated trajectories. Using these decision variables, we then develop an objective function that  mathematically quantifies the value of the model decisions. Finally, we restrict these variables using constraints that force the model to find solutions that are feasible for the MTT problem as we have defined it. A simple model is first developed step by step in the coming sections before generalized formulation is presented. 

\subsection{Decision Variables}
The data association and trajectory estimation problems require unique decision variables. Because these two problems lie in different domains, the variables we use to represent these decisions also differ. First, we introduce \textit{continuous} decision variables $\alpha_{j} \in \mathbb{R}^n$ and $\beta_{j} \in \mathbb{R}^n$ to represent the estimated initial position and velocity, respectively, of each trajectory \textit{j}. In our interpretation of the MTT problem we allow the trajectory parameters to lie anywhere in the real-continuous domain. For the data estimation problem, we wish to assign detections to trajectories, a naturally discrete problem. Therefore, we introduce binary decision variables $y_{itj}$ to indicate whether detection $x_{it}$ is assigned to trajectory \textit{j} or not:

\begin{align}
y_{itj} =
\begin{cases}
1, & \text{if detection $x_{it}$ is assigned to trajectory \textit{j},} \\
0, & \text{otherwise.}
\end{cases}
\end{align}

Next, we use these decision variables to develop an objective function to score the solutions found by the model. 

\subsection{Objective Function}
Here, we would like to develop a function that quantifies the quality of a feasible solution. Our goal in this work is to develop a single model that solves in a single iteration. Thus, this function must produce a single measure of goodness for both the data association and trajectory estimation problems. Such a function should take into account the cost of assigning detections in addition to the cost of the estimated trajectory determined by those assignments. 

The goal of the trajectory estimation problem is to minimize the distance between the positions of the true trajectories and the positions of the estimated trajectories. We do not know the position of the true trajectories but we do have detections $x_{it}$. So our goal is to minimize the distance between the detections and the positions of the estimated trajectories. For ease of notation we denote the estimated position of trajectory \textit{j} at scan \textit{t} as $\hat{x}_{jt}$. Using this notation, the \textit{cost} of the trajectory estimation problem defined as the distance between the detections $x_{it}$ and the estimated positions $\hat{x}_{jt}$. 

Under the current assumptions, in which there is no detection ambiguity, the cost of making a data association is exactly equivalent to the cost of trajectory estimation for a given solution. This is because all detections will be assigned to a trajectory, and in the case of no detection ambiguity all such assignments are weighted evenly. 

In words, we wish to develop a function which returns a cost of $\|x_{it} - \hat{x}_{jt}\|$ when a detection is assigned and 0 otherwise:

\begin{align}
\text{cost} = 
\begin{cases}
\|x_{it} - \hat{x}_{jt}\|, & \text{if detection $x_{it}$ is assigned}\\
				& \text{to trajectory \textit{j},} \\
0, & \text{otherwise.}
\end{cases}
\end{align}

Moving toward a more mathematical representation of this function, we let $\mathcal{A}_{t}$ be the set of pairs that indicate the assignment of detection \textit{i} to trajectory \textit{j} of scan \textit{t}. Then the cost of assigning each detection $x_{it}$ to trajectory $j$, can be written as

\begin{align}
\|\sum_{(i,j)\in \mathcal{A}_{t}} x_{it} - \hat{x}_{jt}\| 
\end{align} 

To get a single measure for the total cost of assigning all detections, we simply sum over all targets and all scans and we arrive at
\begin{align}
\sum_{j=1}^{P} \sum_{t=1}^{T}  \left \|\sum_{(i,j)\in \mathcal{A}_{t}} x_{it} - \hat{x}_{jt}\right \|. 
\end{align}

However, we desire a function in terms of our decision variables. Note that because all detections will be assigned to a trajectory and vice versa, the following equivalence holds:

\begin{align}
\sum_{(i,j)\in \mathcal{A}_{t}} x_{it} = \sum_{i=1}^{P}y_{itj}x_{it}.
\end{align}

Making the appropriate substitutions, we now have:
\begin{align}
 \sum_{j=1}^{P} \sum_{t=1}^{T}  \left \| \sum_{i=1}^{P}y_{itj}x_{it} x_{it} - \hat{x}_{jt} \right \|. 
\end{align}

In terms of our decision variables we also have:
\begin{align}
	\hat{x}_{jt} =  \alpha_{j} + \beta_{j}t.
\end{align}

So we can substitute further to get the following function:
\begin{align}\label{eq:final_function}
\sum_{j=1}^{P} \sum_{t=1}^{T}  \left\| \sum_{i=1}^{P}y_{itj}x_{it} - \alpha_{j} - \beta_{j}t\right\| 
\end{align}

Finally, in order to fully develop an objective function, in which we wish to find the highest quality solution, we will minimize over \eqref{eq:final_function} to get:

\begin{align}\label{eq:full_objective}
\underset{Y_{itj}, \alpha_{j}, \beta_{j}}{\text{minimize: }} &  \sum_{j=1}^{P} \sum_{t=1}^{T}  \left \| \sum_{i=1}^{P}y_{itj}x_{it} -  \alpha_{j} - \beta_{j}t \right \|. 
\end{align}

which finds the lowest cost or highest quality feasible solution of detection assignments.

At this point it is necessary to discuss the advantages and disadvantages of the two natural norms that will be considered, the L1 and L2 norms. The L1 norm has the advantage that it can be reformulated using linear optimization (through the addition of continuous variables and constraints), and it is well known to be more robust to outliers. Furthermore, existing algorithms for MIO are more well developed for linear rather than quadratic optimization. However, the L2 norm square form, which is equivalent to the residual sum of squares (RSS), has the advantage that it can be quickly computed using a matrix formulation, making it more amenable to a heuristic. This concept will be discussed further in \mysection~\ref{\myabrv Heuristic}.

Because of the computational benefits of linear optimization over quadratic optimization, we choose the L1 norm for use in our models. Therefore, we now show how \eqref{eq:full_objective} can be reformulated using linear optimization in the case of the L1 norm by introducing continuous variables $\psi_{jt} \in \mathbb{R}^n$ and the following constraints. 

\begin{align}
\sum_{i=1}^{P}y_{itj}x_{it} - \alpha_{j} - \beta_{j}t \leq \psi_{jt} \qquad \forall j,t\\
-\left(\sum_{i=1}^{P}y_{itj}x_{it} - \alpha_{j} - \beta_{j}t\right) \geq \psi_{jt} \qquad \forall j,t
\end{align}

The resulting objective function for the case of the L1 norm would then be:
\begin{align}
\underset{\psi_{jt}}{\text{minimize: }} & \sum_{j=1}^{P} \sum_{t=1}^{T} \psi_{jt}
\end{align}

For the case of the L2 norm, the objective function would be:
\begin{align}
\underset{\psi_{jt}}{\text{minimize: }} & \sum_{j=1}^{P} \sum_{t=1}^{T} {\|\psi_{jt}\|}^{2}_{2}
\end{align}

\subsection{Constraints}
For each scan, each detection $x_{it}$ must be assigned to exactly one target \textit{j}:
\begin{align}
\sum_{j=1}^{P} y_{itj} = 1 \qquad \forall i,t
\end{align}

Similarly, for each scan, each target must be assigned exactly one detection:
\begin{align}
\sum_{i=1}^{P} y_{itj} = 1 \qquad \forall j,t
\end{align}

\subsection{Simple Formulation}
Combining all of these elements together, we arrive at the following MIO model:
\begin{align*}
\underset{\psi_{jt}}{\text{minimize: }} & \sum_{j=1}^{P} \sum_{t=1}^{T} \psi_{jt} \\
\text{subject to: }	& \sum_{j=1}^{P} y_{itj} = 1 \qquad \forall i,t\\
				& \sum_{i=1}^{P} y_{itj} = 1 \qquad \forall j,t\\
				& \sum_{i=1}^{P}y_{itj}x_{it} - \alpha_{j} - \beta_{j}t \leq \psi_{jt} \qquad \forall j,t\\
				& -\left(\sum_{i=1}^{P}y_{itj}x_{it} - \alpha_{j} - \beta_{j}t\right) \geq \psi_{jt} \qquad \forall j,t\\
			 	& y_{itj} \in \{0,1\} \quad \forall i,t,j\\
				& \alpha_{j} \in \mathbb{R}^n \quad \forall j,\quad \beta_{j} \in \mathbb{R}^n \quad \forall j, \quad z_{jt} \in \mathbb{R}^n \quad \forall j,t
\end{align*}

This formulation is simple in the sense that it involves few variables and constraints, making it highly interpretable and easily implementable. However, it has the disadvantage of being ill suited for extensions to detection ambiguity because it heavily relies on the fact that exactly one of the detections at each scan is associated to a target. This forces the interaction terms of $\sum_{i=1}^{P}y_{itj}x_{it}$ to always be nonzero. However, would be incorrect in the case of detection ambiguity. Therefore, there is a need to generalize this formulation to one that is amenable to scenarios with detection ambiguity, and this is presented next.

\subsection{Generalized Formulation}
We desire a formulation that can more easily extend to detection ambiguity. To this end, we introduce a new variable $z_{jt}$ which takes on the value $x_{it}$ when $y_{ijt}=1$ and some arbitrary number when $y_{itj}=0$. Using this method we must adjust \label{eq:simple_objective} as follows:

\begin{align}\label{eq:generalized_objective}
\underset{z_{jt}, \alpha_{j}, \beta_{j}}{\text{minimize: }} & \sum_{j=1}^{P} \sum_{t=1}^{T} \|z_{jt} - \alpha_{j} - \beta_{j}t\|
\end{align}

This objective can then be linearized by again introducing $\theta{jt}$ and similar constraints as follows. 
\begin{align}\label{eq:generalized_linear_objective}
\underset{\psi_{jt}}{\text{minimize: }} & \sum_{j=1}^{P} \sum_{t=1}^{T} \psi_{jt}
\end{align}
\begin{align}
z_{jt} - \alpha_{j} - \beta_{j}t \leq \psi_{jt} \qquad \forall i,j,t\\
-(z_{jt} - \alpha_{j} - \beta_{j}t) \geq \psi_{jt} \qquad \forall i,j,t
\end{align}
 
Furthermore, we must ensure that the decision variable $z_{jt}$ will only take on the value of $x_{it}$ in the objective function if $x_{it}$ is assigned to target \textit{j} ($y_{itj} = 1$). We enforce this effect using the following constraint:

\begin{align}
M_{t}(1-y_{itj}) \geq |z_{jt} - x_{it}y_{itj}| \qquad \forall i,t,j\\
\end{align}

where $M_{t} = \underset{j}{\text{max}}|x_{it}|$ for each scan. Furthermore, we can write this equivalently as a linear optimization problem by using the following set of two linear constraints:

\begin{align}
x_{it}y_{itj} + M_{t}(1-y_{itj}) \geq z_{jt} \qquad \forall i,t,j\\
x_{it}y_{itj} - M_{t}(1-y_{itj}) \leq z_{jt} \qquad \forall i,t,j
\end{align}

Combining all of these elements together, we arrive at the following generalized MIO model:
\begin{align*}
\underset{\psi_{jt}}{\text{minimize: }} & \sum_{j=1}^{P} \sum_{t=1}^{T} \psi_{jt} \\
\text{subject to: }	& \sum_{j=1}^{P} y_{itj} = 1 \qquad \forall i,t\\
				& \sum_{i=1}^{P} y_{itj} = 1 \qquad \forall j,t\\
				& x_{it}y_{itj} + M_{t}(1-y_{itj}) \geq z_{jt} \qquad \forall i,t,j\\
				& x_{it}y_{itj} - M_{t}(1-y_{itj}) \leq z_{jt} \qquad \forall i,t,j\\
				& z_{jt} - \alpha_{j} - \beta_{j}t \leq \psi_{jt} \qquad \forall i,j,t\\
				& -(z_{jt} - \alpha_{j} - \beta_{j}t) \geq \psi_{jt} \qquad \forall i,j,t\\
			 	& y_{itj} \in \{0,1\} \quad \forall i,t,j\\
				& \alpha_{j} \in \mathbb{R}^n \quad \forall j,\quad \beta_{j} \in \mathbb{R}^n \quad \forall j, \quad z_{jt} \in \mathbb{R}^n \quad \forall j,t
\end{align*}
