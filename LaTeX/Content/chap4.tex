In this \mytitle, we present a detailed description of a local search heuristic that finds high quality feasible solutions to \eqref{eq:generalized_problem}, which we call the basic MIO model. These solutions can be used as a warm start to the basic MIO model, providing a performance boost in run time and improved solution quality. The heuristic utilizes randomized local search methods to find locally optimal solutions. Fundamentally, randomized local search methods begin with a random initial starting point and converge to a locally optimal solution through local improvements. Applying this scheme to a growing number of randomized starting points increases the probability of reaching high quality solutions or even the globally optimal solution.

We begin by describing the heuristic mechanism for a single starting point. A \textit{starting point} is a randomized solution to the association problem that satisfies the assignment equations \eqref{eq:all_detections} and \eqref{eq:all_targets}. To generate a random starting point, associations are randomly assigned to detections for each scan, in such a way that each ordered association, or permutation, has an equal probability of occurring. The assignment cost, temporarily denoted by $f$, of this starting point is then calculated by solving \eqref{eq:assignment_cost} for all scans of each trajectory. After initializing with a starting point, the heuristic begins a \textit{sweep} through the scans, i.e., a single pass through the scans. At each scan of the sweep two detections are randomly selected from the current scan and their assignments are exchanged, an operation referred to as a \textit{swap}. this process generates a new feasible solution. The assignment cost of this new solution is calculated, and the swap is kept if the cost of the new solution improves. Otherwise the swap is rejected. The heuristic continues to conduct sweeps until a full sweep is completed without accepting a single swap, at which point it terminates. Pseudocode for this local search heuristic, referred to as the \textit{basic} heuristic, is provided in Figure~\ref{fig:Basic_Heuristic}.
\begin{figure}[ht] 
 \removelatexerror
\begin{algorithm}[H]
 \caption{Randomized local search with heuristic swaps}
 \label{alg:Basic_Heuristic}
 \begin{algorithmic}[1]
  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
 \REQUIRE $\boldsymbol{\mathcal{X}}$, P, T
 \ENSURE  $f$, $y$
 \\ \textit{Initialization} : Assign random initial assignment to $y^{0}$
  \STATE Calculate $\alpha_{j}, \beta_{j} \quad \forall j $
  \STATE Calculate $f^{0}$ - the cost of assignment $y^{0}$
  \STATE swapped $\leftarrow true$
  \STATE $k\leftarrow1$
  \WHILE{swapped}
  \STATE swapped $\leftarrow false$
  \FOR{$t$ in $\{t_{1},t_{2},...,T\}$}
  \STATE Randomly choose $j,m\in\{1,\ldots,P\}$
  \STATE Find $i,l$ such that $y^{k-1}_{itm}=1$ and $y^{k-1}_{ltj}=1$
  \STATE Swap such that $y^{k}_{itj}\leftarrow1$, $y^{k-1}_{itm}\leftarrow 0$, $y^{k}_{ltm}\leftarrow1$ and $y^{k}_{ltj}\leftarrow0$
  \STATE Calculate $f^{k}$ the cost of assignment $y^k$ as well as $\alpha_{j}, \beta_{j}, \alpha_{m}, \beta_{m}$
  \IF {($f^{k} \geq f^{k-1}$)}
  \STATE $y^{k} \leftarrow y^{k-1}$
  \ELSE 
  \STATE swapped $\leftarrow true$
  \ENDIF
  \ENDFOR
  \STATE $ k \leftarrow k + 1 $
  \ENDWHILE
 \RETURN $f^{k}, y^{k}$ 
 \end{algorithmic} 
 \end{algorithm}
  \caption{Pseudocode for heuristic for a single starting point.}
  \label{fig:Basic_Heuristic}
 \end{figure}

The goal of any heuristic is to find good feasible solutions in an efficient manner. In particular, the goal of our heuristic is to find good feasible solutions that can serve as a warm start for the basic MIO model. In \mysection~\ref{\myabrv Basic MIO Model} we discussed our choice to use the $\ell_1$ norm over the $\ell_2$ norm for use in the objective function of our MIO models. We now turn to discuss why the $\ell_2$ norm is the preferred choice for use in this heuristic. The two main areas of concern are 1) efficiency of the algorithm and 2) quality of the solution.

In the case of the MIO models, the preferred objective function utilizes the $\ell_1$ norm because it lends itself easily to linear optimization solvers which have known performance advantages over quadratic optimization solvers. However, in the case of the heuristic, the objective function no longer needs to determine the associations because they are predetermined by the initialization and swapping processes. In addition, the heuristic objective score needs to be recomputed after each swap, even if it is eventually rejected, and hundreds to thousands of swaps may be carried out for a single starting point. This fact makes the computational cost of this calculation critical to the scalability of the heuristic.  Computing the $\ell_1$ norm objective would require solving an linear optimization problem. Even though this linear optimization problem can be computed quite quickly by state of the art optimization solvers, the $\ell_2$ norm squared, or RSS, can be computed by simple linear algebra in a fraction of the time, as shown in \cite{RSS-Matrix}. Therefore, with respect to efficiency, the $\ell_2$ norm is the clear choice for use in the local search heuristic.

In judging the quality of a heuristic solution, it is important to remember that it will serve as a warm start for the MIO, which uses the $\ell_1$ norm in its objective. It is natural to assume that using the same norm for the heuristic objective would lead to higher quality solutions. While this holds true, we found that both norms find good quality solutions, likely due to the fact that they both represent measures of distance, making them highly correlated. Thus, the choice of the $\ell_2$ over the $\ell_1$ norm might not significantly degrade the solution quality. Although the use of $\ell_2$ norm runs the risk of obtaining solutions which are not necessarily extremely good solutions for the $\ell_1$ norm objective the potential loss in solution quality is far outweighed by the guaranteed efficiency improvements afforded by the $\ell_2$ norm. 

Furthermore, the local search heuristic can be parallelized by running unique starting points on separate processors, enabling the ability to run a larger number of starting points without increasing the run time. Increasing the number of starting points greatly increases the potential for the heuristic to improve solution quality and thus further reducing the risk of reduced solution quality. Therefore, we make the choice to use the $\ell_2$ norm in the objective function of the heuristic.