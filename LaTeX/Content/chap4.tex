In this section, we present a detailed description of a heuristic that finds high quality feasible solutions for problem \eqref{eq:Basic_Problem}. These solutions can be used as a warm start to the MIO, providing a performance boost to the MIO. The heuristic leverages the power of randomized local search methods to find locally optimal solutions. 
%Although the heuristic takes a local search approach, we hypothesize that it will discover near optimal solutions, provided that it is initialized with enough random starting points. 

The fundamental concept behind randomized local search methods is to begin with a randomized starting point and through local improvements converge to a locally optimal solution. By applying this scheme to a growing number of randomized starting points, the probability of reaching high quality solutions, or even the globally optimal solution, increases.

We now detail the heuristic mechanism for a single starting point. The heuristic \textit{starting point} is a randomized solution to the assignment problem, that satisfies the assignment equations \eqref{eq:all_detections} and \eqref{eq:all_targets}. This is done for each scan, by choosing a permutation over the detections uniformaly at random, and associating to trajectories by the order of the permutation, i.e., if in the random permutation detection $i$ is at location $j$ than it will be assigned to trajectory $j$. The assignemnt cost of this starting point is then calculated by solving \eqref{eq:assignemnt_cost}. 
After initializing the hueristic starts a \textit{sweep} through the scans, i.e., a single pass through the scans. At each stage of the sweep two detections are randomly selected from the current scan and they exchange assignments, an operation we call a \textit{swap}, generating a new feasible solution. The assignment cost of this new solution is calculated, and if it improves the current cost it is kept and otherwise it is rejected. The hueristic will continue to conduct sweeps, until a full sweep is completed without accepting a single swap, in which point it terminates. The full description of the heuristic algorithm for a single starting point is given in Algorithm~\ref{alg:Basic_Heuristic}.



%The heuristic then  randomly selects two detections from the first scan and exchanges their assignments. This random exchanged is referred to as a \textit{swap}. The objective score of the new solution is calculated. If the objective score improves, the swap is kept, otherwise it is rejected. The heuristic then advances to the next scan where the same process is repeated, until it reaches the last scan. This completes a single \textit{sweep}, i.e., a single pass through all scans.  The heuristic continues sweeping through all scans, randomly swapping detections and only keeping the swap if the solution improves, until it makes a single sweep without accepting a single swap. At this point the heuristic terminates and the final assignments are obtained. Pseudocode for this proposed heuristic is detailed below in Algorithm~\ref{alg:Basic_Heuristic}.


%\begin{figure}
\begin{algorithm}
 \caption{Randomized local search with heuristic swaps}
 \label{alg:Basic_Heuristic}
 \begin{algorithmic}[1]
  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
 \REQUIRE $\boldsymbol{\mathcal{X}}$, P, T
 \ENSURE  $F$, $y_{itj}$
 \\ \textit{Initialization} : Assign random initial assignments for $y^{0}_{itj}$
  \STATE Calculate $\alpha_{j}, \beta_{j} \quad \forall j $
  \STATE Calculate $F^{0}$
  \STATE swapped $\leftarrow true$
  \STATE $k\leftarrow1$
  \WHILE{swapped}
  \STATE swapped $\leftarrow false$
  \FOR{$t$ in $\{t_{1},t_{2},...,T\}$}
  \STATE Randomly choose $j,m\in\{1,\ldots,P\}$
  \STATE Find $i,l$ such that $y^{k-1}_{itm}\leftarrow1$ and $y^{k-1}_{ltj}\leftarrow1$
  \STATE Swap such that $y^{k}_{itj}\leftarrow1$ and $y^{k}_{ltm}\leftarrow1$
  \STATE Calculate $F^{k}, \alpha_{j}, \beta_{j}, \alpha_{m}, \beta_{m}$
  \IF {($F^{k} \geq F^{k-1}$)}
  \STATE $y^{k} \leftarrow y^{k-1}$
  \ELSE 
  \STATE swapped $\leftarrow true$
  \ENDIF
  \ENDFOR
  \STATE $ k \leftarrow k + 1 $
  \ENDWHILE
 \RETURN $F^{k}, y^{k}_{itj}$ 
 \end{algorithmic} 
 \end{algorithm}

The goal of any heuristic is to find good feasible solutions in an efficient manner. In particular, the goal of our heuristic is to find good feasible solution for the MIO formulation, which can serve as a warm start for the MIO. In \mysection~\ref{\myabrv Basic MIO Model} we discussed our choice to use the $\ell_1$ norm over the $\ell_2$ norm for use in the objective of our MIO models. We now turn to discuss why the $\ell_2$ is the preferred choice for use in this heuristic. The two main ares of concern are 1) efficiency of the algorithm and 2) quality of the solution.

 In the case of the MIO, the preferred objective function was the  $\ell_1$ norm because it lent itself easily to linear optimization solvers which have known performance advantages over quadratic optimization solvers. However, in the case of the heuristic, the objective function is computed given an assignment, i.e., we need to solve problem \eqref{eq:assignemnt_cost}, to obtain the specific assignment's cost, rather than the more difficult problem \eqref{eq:full_objective}. Furthermore, this objective needs to be recomputed after each swap, even if it is eventually rejected, and hundreds to thousands of swaps may be carried out for a single starting point. This fact makes the computational cost of this calculation critical to the scalability of the heuristic.  Computing the $\ell_1$ norm objective would require solving an linear optimization problem. Even though this linear optimization problem can be computed quite quickly by state of the art optimization solvers, the $\ell_2$ norm squared, or RSS, can be computed by simple linear algebra in a fraction of the time, as shown in \cite{RSS-Matrix}. Therefore, with respect to efficiency, the $\ell_2$ norm is the clear choice for use in the heuristic.

When judging the quality of the heuristic solution, we must look at its purpose. Since it will serve as a warm start for the MIO, which uses the $\ell_1$ norm in its objective, we would assume that better solutions will be obtained by using the same norm for the heuristic objective. However, the heuristic will converge to high quality solutions, and since both norms represent measures of distance, and hence are correlated, we assume that a high quality solution as measured by the $\ell_2$ norm is also a high quality solution as measured by the $\ell_1$ norm. Thus, the choice of the $\ell_2$ over the $\ell_1$ norm might not significantly degrade the solution quality.

%The choice of objective is less clear in regards to quality of the solution. We previously noted that the $\ell_1$ norm is more robust to outliers than the $\ell_2$ norm, but this does not mean the $\ell_2$ norm cannot still find good feasible solutions. In fact, both norms represent measures of distance and are highly correlated. Therefore, it follows that a high quality solution as measured by the $\ell_2$ norm is also a high quality solution as measured by the $\ell_1$ norm, although their optimal solution may differ. It is important to remember that the goal of the heuristic is merely to find high quality solutions and let the power of optimization do the rest. 

A final note on the heuristic is that it can be parallelized by running partitions of the $N$ starting points on separate cores, leading to even greater performance advantages. This concept, along with its performance advantages, will be discussed at length in \mysection~\ref{\myabrv Results}. 
>>>>>>> Stashed changes
